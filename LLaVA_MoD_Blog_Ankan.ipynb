{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9d0bf7a2",
      "metadata": {
        "id": "9d0bf7a2"
      },
      "source": [
        "# Making LLaVA Tiny via MoE Knowledge Distillation\n",
        "### Blog by Ankan Das"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17e042ef",
      "metadata": {
        "id": "17e042ef"
      },
      "source": [
        "## Motivation  \n",
        "I‚Äôve always been fascinated by multimodal models that can interpret both images and text. But most of them are huge and impractical to deploy in real-world devices.  \n",
        "This paper caught my attention for its smart use of Mixture-of-Experts (MoE) and distillation to make a model like LLaVA tiny and efficient ‚Äî without losing performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2140c30",
      "metadata": {
        "id": "b2140c30"
      },
      "source": [
        "## Historical Context  \n",
        "Multimodal models like CLIP, Flamingo, and LLaVA have shown remarkable performance in combining vision and language.  \n",
        "However, their size limits real-world usability. LLaVA-MoD is a response to this ‚Äî achieving better results with fewer parameters and less data, using MoE and distillation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5811ad95",
      "metadata": {
        "id": "5811ad95"
      },
      "source": [
        "## What I Learned from the Paper  \n",
        "\n",
        "### üìå Architecture Overview  \n",
        "The model follows this sequence:  \n",
        "`Image ‚Üí CLIP Vision Encoder (frozen) ‚Üí Vision-Language Adapter ‚Üê Text`  \n",
        "‚Üí Combined features go into a Language Model with Mixture-of-Experts (MoE).  \n",
        "\n",
        "### üìå What is MoE?  \n",
        "Instead of using one feed-forward block for every token, the MoE approach uses a **router** to select the best 2 experts for each token ‚Äî reducing computation.  \n",
        "\n",
        "### üìå Progressive Distillation  \n",
        "1. **Mimic Distillation** ‚Äì Student mimics teacher outputs using KL loss.  \n",
        "2. **Preference Distillation** ‚Äì Student is shown good and bad responses, and learns to prefer the better one.  \n",
        "This helps avoid hallucinations and improves reasoning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5b422bc",
      "metadata": {
        "id": "d5b422bc"
      },
      "source": [
        "## Code / Notebook  \n",
        "\n",
        "While I didn‚Äôt replicate the full model due to compute limitations, here's a simple simulation to show how top-2 expert routing might work:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b93e416f",
      "metadata": {
        "id": "b93e416f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Simulate router scores for 4 experts\n",
        "scores = np.array([0.2, 0.5, 0.1, 0.9])\n",
        "top_k = scores.argsort()[-2:][::-1]\n",
        "\n",
        "print(\"Top-2 selected experts (highest scoring):\", top_k)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9938974",
      "metadata": {
        "id": "b9938974"
      },
      "source": [
        "## Reflections  \n",
        "\n",
        "**(a) What surprised me:**  \n",
        "How effective preference distillation was in reducing hallucinations, even better than instruction tuning in some cases.  \n",
        "\n",
        "**(b) Scope for improvement:**  \n",
        "The MoE model could be made even lighter by using quantization or low-rank approximation. It would also be interesting to try this architecture on audio+text data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db234d62",
      "metadata": {
        "id": "db234d62"
      },
      "source": [
        "## References  \n",
        "- [LLaVA-MoD Paper (ResearchGate)](https://www.researchgate.net/publication/383494370_LLaVA-MoD_Making_LLaVA_Tiny_via_MoE_Knowledge_Distillation)  \n",
        "- [GitHub Repo](https://github.com/shufangxun/LLaVA-MoD)  \n",
        "- Hugging Face Transformers  \n",
        "- CLIP Model  \n",
        "- Qwen-VL Documentation"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}