{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8f6ad9d3",
      "metadata": {
        "id": "8f6ad9d3"
      },
      "source": [
        "# Making LLaVA Tiny via MoE Knowledge Distillation\n",
        "### Blog by Ankan Das"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2640c76a",
      "metadata": {
        "id": "2640c76a"
      },
      "source": [
        "## Motivation  \n",
        "I‚Äôve always been fascinated by multimodal models that can interpret both images and text. But most of them are huge and impractical to deploy in real-world devices.  \n",
        "This paper caught my attention for its smart use of Mixture-of-Experts (MoE) and distillation to make a model like LLaVA tiny and efficient ‚Äî without losing performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3480d488",
      "metadata": {
        "id": "3480d488"
      },
      "source": [
        "## Historical Context  \n",
        "Multimodal models like CLIP, Flamingo, and LLaVA have shown remarkable performance in combining vision and language.  \n",
        "However, their size limits real-world usability. LLaVA-MoD is a response to this ‚Äî achieving better results with fewer parameters and less data, using MoE and distillation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47ff5d7d",
      "metadata": {
        "id": "47ff5d7d"
      },
      "source": [
        "## What I Learned from the Paper  \n",
        "\n",
        "### üìå Architecture Overview  \n",
        "The model follows this sequence:  \n",
        "`Image ‚Üí CLIP Vision Encoder (frozen) ‚Üí Vision-Language Adapter ‚Üê Text`  \n",
        "‚Üí Combined features go into a Language Model with Mixture-of-Experts (MoE).  \n",
        "\n",
        "### üìå What is MoE?  \n",
        "Instead of using one feed-forward block for every token, the MoE approach uses a **router** to select the best 2 experts for each token ‚Äî reducing computation.  \n",
        "\n",
        "### üìå Progressive Distillation  \n",
        "1. **Mimic Distillation** ‚Äì Student mimics teacher outputs using KL loss.  \n",
        "2. **Preference Distillation** ‚Äì Student is shown good and bad responses, and learns to prefer the better one.  \n",
        "This helps avoid hallucinations and improves reasoning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8e0a5b3",
      "metadata": {
        "id": "c8e0a5b3"
      },
      "source": [
        "### Simulating Top-k Routing with Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "676bcf44",
      "metadata": {
        "id": "676bcf44"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Simulate router scores for 4 experts\n",
        "scores = np.array([0.2, 0.5, 0.1, 0.9])\n",
        "top_k = scores.argsort()[-2:][::-1]\n",
        "\n",
        "print(\"Top-2 selected experts (highest scoring):\", top_k)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9664b6d9",
      "metadata": {
        "id": "9664b6d9"
      },
      "source": [
        "### Simulating Preference Distillation Logic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1396b000",
      "metadata": {
        "id": "1396b000"
      },
      "outputs": [],
      "source": [
        "# Simulate preference scoring in distillation\n",
        "\n",
        "teacher_scores = {\n",
        "    \"Answer_A+\": 0.85,  # Preferred answer\n",
        "    \"Answer_A-\": 0.45   # Less preferred\n",
        "}\n",
        "\n",
        "student_scores = {\n",
        "    \"Answer_A+\": 0.60,\n",
        "    \"Answer_A-\": 0.55\n",
        "}\n",
        "\n",
        "# Preference loss logic: Encourage higher score for A+ than A-\n",
        "preference_margin = teacher_scores[\"Answer_A+\"] - teacher_scores[\"Answer_A-\"]\n",
        "student_margin = student_scores[\"Answer_A+\"] - student_scores[\"Answer_A-\"]\n",
        "\n",
        "print(\"Teacher preference margin:\", preference_margin)\n",
        "print(\"Student preference margin:\", student_margin)\n",
        "\n",
        "if student_margin < preference_margin:\n",
        "    print(\"Apply loss to reinforce preference toward A+\")\n",
        "else:\n",
        "    print(\"No adjustment needed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6341f85e",
      "metadata": {
        "id": "6341f85e"
      },
      "source": [
        "## Reflections  \n",
        "\n",
        "**(a) What surprised me:**  \n",
        "How effective preference distillation was in reducing hallucinations, even better than instruction tuning in some cases.  \n",
        "\n",
        "**(b) Scope for improvement:**  \n",
        "The MoE model could be made even lighter by using quantization or low-rank approximation. It would also be interesting to try this architecture on audio+text data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45ed0ef6",
      "metadata": {
        "id": "45ed0ef6"
      },
      "source": [
        "## References  \n",
        "- [LLaVA-MoD Paper (ResearchGate)](https://www.researchgate.net/publication/383494370_LLaVA-MoD_Making_LLaVA_Tiny_via_MoE_Knowledge_Distillation)  \n",
        "- [GitHub Repo](https://github.com/shufangxun/LLaVA-MoD)  \n",
        "- Hugging Face Transformers  \n",
        "- CLIP Model  \n",
        "- Qwen-VL Documentation"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}