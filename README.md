
# LLaVA-MoD: Making LLaVA Tiny via MoE Knowledge Distillation

This repository contains the course project for DA623 (Winter 2025) by **Ankan Das**.

The project explores the paper [LLaVA-MoD](https://www.researchgate.net/publication/383494370_LLaVA-MoD_Making_LLaVA_Tiny_via_MoE_Knowledge_Distillation), which presents a lightweight vision-language model using Mixture-of-Experts and progressive knowledge distillation.

## ðŸ“˜ Contents

- `LLaVA_MoD_Blog_Ankan.ipynb` â€” Blog-style explanation covering:
  - Motivation behind the project
  - Architectural breakdown of LLaVA-MoD
  - Key insights and learning from the paper
  - MoE routing demo with code
  - Reflections and future possibilities

## ðŸ”— References

- [Paper on Research Gate](https://www.researchgate.net/publication/383494370_LLaVA-MoD_Making_LLaVA_Tiny_via_MoE_Knowledge_Distillation)
- [Official GitHub Repo](https://github.com/shufangxun/LLaVA-MoD)  
- [CLIP by OpenAI](https://github.com/openai/CLIP)  
- [Qwen-VL](https://huggingface.co/Qwen/Qwen-VL)

## ðŸ§  Author

Ankan Das â€” M.Tech (Robotics & AI), IIT Guwahati
