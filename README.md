
# LLaVA-MoD: Making LLaVA Tiny via MoE Knowledge Distillation

This repository contains the course project for DA623 (Winter 2025) by **Ankan Das**.

The project explores the paper [LLaVA-MoD](https://arxiv.org/abs/2403.06878), which presents a lightweight vision-language model using Mixture-of-Experts and progressive knowledge distillation.

## ðŸ“˜ Contents

- `notebook/LLaVA_MoD_Blog_Ankan.ipynb` â€” Blog-style explanation covering:
  - Motivation behind the project
  - Architectural breakdown of LLaVA-MoD
  - Key insights and learning from the paper
  - MoE routing demo with code
  - Reflections and future possibilities

## ðŸ”— References

- [Paper on arXiv](https://arxiv.org/abs/2403.06878)  
- [Official GitHub Repo](https://github.com/shufangxun/LLaVA-MoD)  
- [CLIP by OpenAI](https://github.com/openai/CLIP)  
- [Qwen-VL](https://huggingface.co/Qwen/Qwen-VL)

## ðŸ§  Author

Ankan Das â€” M.Tech (Robotics & AI), IIT Guwahati
